{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, cmap = None, fig_size = (10, 10)):\n",
    "    fig, ax = plt.subplots(figsize=fig_size)\n",
    "    ax.imshow(image, cmap = cmap)\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('../img/seal.png')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (7, 7), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(np.flip(image, axis = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Simple Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(T, threshInv) = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "# Threshold Binary Inverse\n",
    "show_image(threshInv, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using normal thresholding (rather than inverse thresholding),\n",
    "# we can change the last argument in the function to make the coins\n",
    "# black rather than white.\n",
    "(T, thresh) = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY)\n",
    "# Threshold Binary\n",
    "show_image(thresh, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we can visualize only the masked regions in the image\n",
    "bitwise_output = cv2.bitwise_and(image, image, mask=threshInv)\n",
    "show_image(np.flip(image, axis = 2))\n",
    "show_image(np.flip(bitwise_output, axis = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Otsu's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply Otsu's automatic thresholding -- Otsu's method automatically\n",
    "# determines the best threshold value `T` for us\n",
    "(T, threshInv) = cv2.threshold(blurred, 0, 255,\n",
    "    cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "print(\"Otsu's thresholding value: {}\".format(T))\n",
    "show_image(threshInv, cmap = 'gray') # same as cv2.imshow(\"Threshold\", threshInv)\n",
    " \n",
    "# finally, we can visualize only the masked regions in the image\n",
    "bitwise_and = cv2.bitwise_and(image, image, mask=threshInv)\n",
    "show_image(bitwise_and, cmap = 'gray') # same as cv2.imshow(\"Output\", bitwise_and)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Adaptive Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of manually specifying the threshold value, we can use adaptive\n",
    "# thresholding to examine neighborhoods of pixels and adaptively threshold\n",
    "# each neighborhood -- in this example, we'll calculate the mean value\n",
    "# of the neighborhood area of 25 pixels and threshold based on that value;\n",
    "# finally, our constant C is subtracted from the mean calculation (in this\n",
    "# case 15)\n",
    "thresh = cv2.adaptiveThreshold(blurred, 255,\n",
    "    cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 35, 5)\n",
    "show_image(thresh, cmap = 'gray') # same as cv2.imshow(\"OpenCV Mean Thresh\", thresh)\n",
    " \n",
    "# personally, I prefer the scikit-image adaptive thresholding, it just\n",
    "# feels a lot more \"Pythonic\"\n",
    "T = skimage.filters.threshold_local(blurred, 33, offset=4, method=\"gaussian\")\n",
    "thresh = (blurred < T).astype(\"uint8\") * 255\n",
    "show_image(thresh, cmap = 'gray') # cv2.imshow(\"scikit-image Mean Thresh\", thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Apply all three types of thresholding to another image - Which one has performmed better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply the above three thresholding methods to another image. Which has performed better? - You may need to fine tune Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Sobel and Scharr Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread('../img/bricks.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# compute gradients along the X and Y axis, respectively\n",
    "sobelx = cv2.Sobel(gray, ddepth = cv2.CV_64F, dx=1, dy=0)\n",
    "sobely = cv2.Sobel(gray, ddepth = cv2.CV_64F, dx=0, dy=1)\n",
    "\n",
    "# the gx and gy images are now of the floating point data type\n",
    "# so we need to take care to convert them back to an unsigned 8-bit\n",
    "# integer representation so other opencv functions can utilise them\n",
    "sobelx = cv2.convertScaleAbs(sobelx)\n",
    "sobely = cv2.convertScaleAbs(sobely)\n",
    "sobel_combined = cv2.addWeighted(sobelx, 0.5, sobely, 0.5, 0)\n",
    "\n",
    "# Compute combined gradients using Laplacian Algorithm (and convert floating values to ints)\n",
    "laplacian = cv2.Laplacian(gray,cv2.CV_64F)\n",
    "laplacian = cv2.convertScaleAbs(laplacian)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2,3,1),plt.imshow(gray,cmap = 'gray')\n",
    "plt.title('Original'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(2,3,2),plt.imshow(sobelx,cmap = 'gray')\n",
    "plt.title('Sobel X'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(2,3,3),plt.imshow(sobely,cmap = 'gray')\n",
    "plt.title('Sobel Y'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(2,3,4),plt.imshow(sobel_combined,cmap = 'gray')\n",
    "plt.title('Sobel Combined'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(2,3,5),plt.imshow(laplacian,cmap = 'gray')\n",
    "plt.title('Laplacian'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Output Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../img/bricks.png',0)[0:200, 0:200]\n",
    "\n",
    "# Output dtype = cv2.CV_8U\n",
    "sobelx8u = cv2.Sobel(img,cv2.CV_8U,1,0)\n",
    "\n",
    "# Output dtype = cv2.CV_64F. Then take its absolute and convert to cv2.CV_8U\n",
    "sobelx64f = cv2.Sobel(img,cv2.CV_64F,1,0)\n",
    "abs_sobel64f = np.absolute(sobelx64f)\n",
    "sobel_8u = np.uint8(abs_sobel64f)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,3,1),plt.imshow(img,cmap = 'gray')\n",
    "plt.title('Original'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(1,3,2),plt.imshow(sobelx8u,cmap = 'gray')\n",
    "plt.title('Sobel CV_8U'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(1,3,3),plt.imshow(sobel_8u,cmap = 'gray')\n",
    "plt.title('Sobel abs(CV_64F)'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Change kernel sizes in the above Sobel functions. What happens? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Gradient Orientation and Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../img/bricks.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# compute gradients along the X and Y axis, respectively\n",
    "gX = cv2.Sobel(gray, cv2.CV_64F, 1, 0)\n",
    "gY = cv2.Sobel(gray, cv2.CV_64F, 0, 1)\n",
    " \n",
    "# compute the gradient magnitude and orientation respectively\n",
    "mag = np.sqrt((gX ** 2) + (gY ** 2))\n",
    "orientation = np.arctan2(gY, gX) * (180 / np.pi) % 180\n",
    "\n",
    "# Printing magnitude and orientation at pixel (100, 100)\n",
    "print('Magnitude at pixel (100, 100):', mag[100, 100])\n",
    "print('Orientation at pixel (100, 100):', orientation[100, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Detecting Gradient Changes using Orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_angle = 90\n",
    "upper_angle = 100\n",
    "\n",
    "# find all pixels that are within the upper and low angle boundaries\n",
    "idxs = np.where(orientation >= lower_angle, orientation, -1)\n",
    "idxs = np.where(orientation <= upper_angle, idxs, -1)\n",
    "mask = np.zeros(gray.shape, dtype=\"uint8\")\n",
    "mask[idxs > -1] = 255\n",
    " \n",
    "# show the images\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1),plt.imshow(gray,cmap = 'gray')\n",
    "plt.title('Original'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(1,2,2),plt.imshow(mask,cmap = 'gray')\n",
    "plt.title('Mask'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Calculating Orientation and Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the seal image and calculate the gradient orientation and magnitude of the image at pixel 100, 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Canny Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"../img/bricks.png\")\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray, 10, 250)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(121),plt.imshow(gray,cmap = 'gray')\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(edges,cmap = 'gray')\n",
    "plt.title('Canny Edge Image'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tuning Edge Detection Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image, convert it to grayscale, and blur it slightly\n",
    "image = cv2.imread('../img/bricks.png')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "# compute a \"wide\", \"mid-range\", and \"tight\" threshold for the edges\n",
    "wide = cv2.Canny(blurred, 10, 200)\n",
    "mid = cv2.Canny(blurred, 30, 150)\n",
    "tight = cv2.Canny(blurred, 240, 250)\n",
    " \n",
    "# show the edge maps\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(221),plt.imshow(blurred,cmap = 'gray')\n",
    "plt.title('Blurred Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(222),plt.imshow(wide,cmap = 'gray')\n",
    "plt.title('Wide Edge Map'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(223),plt.imshow(mid,cmap = 'gray')\n",
    "plt.title('Mid Edge Map'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(224),plt.imshow(tight,cmap = 'gray')\n",
    "plt.title('Tight Edge Map'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_canny(image, sigma=0.33):\n",
    "    # compute the median of the single channel pixel intensities\n",
    "    v = np.median(image)\n",
    " \n",
    "    # apply automatic Canny edge detection using the computed median\n",
    "    lower = int(max(0, (1.0 - sigma) * v))\n",
    "    upper = int(min(255, (1.0 + sigma) * v))\n",
    "    edged = cv2.Canny(image, lower, upper)\n",
    " \n",
    "    # return the edged image\n",
    "    return edged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = auto_canny(blurred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the image\n",
    "show_image(auto, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Apply both kernels to an image - Which one has performmed better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
